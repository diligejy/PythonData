{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NewsCategorization.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"0v_jiHppBLlK","colab_type":"code","outputId":"a85b11c9-c5a8-4839-88f5-ad735c965918","executionInfo":{"status":"ok","timestamp":1562381843987,"user_tz":-540,"elapsed":3199,"user":{"displayName":"jinyoung song","photoUrl":"","userId":"03900198277628231796"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Google 드라이브를 마운트하려면 이 셀을 실행하세요.\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RfceOM28BNNV","colab_type":"code","outputId":"e9ccb660-9a4a-4e4b-d6a6-6afed07890cc","executionInfo":{"status":"ok","timestamp":1562383847534,"user_tz":-540,"elapsed":7224,"user":{"displayName":"jinyoung song","photoUrl":"","userId":"03900198277628231796"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# 파일 불러오기\n","\n","import os\n","\n","def get_file_list(dir_name):\n","  return os.listdir(dir_name)\n","\n","def get_contents(file_list):\n","  y_class = []\n","  X_text = []\n","  class_dict = {\n","      1: '0', 2:'0', 3:'0', 4:'0', 5: '1', 6:'1', 7:'1', 8:'1'}\n","  \n","  for file_name in file_list:\n","    try:\n","      f = open(file_name, 'r', encoding = 'cp949')\n","      category = int(file_name.split(os.sep)[6].split(\"_\")[0])\n","      y_class.append(class_dict[category])\n","      X_text.append(f.read())\n","      f.close()\n","    except UnicodeDecodeError as e:\n","      print(e)\n","      print(file_name)\n","  return X_text, y_class\n","\n","def get_cleaned_text(text):\n","  import re\n","  text = re.sub('\\W+','', text.lower() )\n","  return text\n","\n","def get_corpus_dict(text):\n","  text = [sentence.split() for sentence in text]\n","  cleaned_words = [get_cleaned_text(word) for words in text for word in words]\n","  \n","  from collections import OrderedDict\n","  corpus_dict = OrderedDict()\n","  for i, v in enumerate(set(cleaned_words)):\n","    corpus_dict[v] = i\n","  return corpus_dict\n","\n","def get_count_vector(text, corpus):\n","  text = [sentence.split() for sentence in text]\n","  word_number_list = [[corpus[get_cleaned_text(word)]for word in words] for words in text]\n","  X_vector = [[0 for _ in range(len(corpus))] for x in range(len(text))] \n","\n","  for i, text in enumerate(word_number_list):\n","    for word_number in text:\n","      X_vector[i][word_number] += 1\n","      \n","  return X_vector\n","\n","import math\n","def get_cosine_similarity(v1, v2):\n","  \"compute cosine similarity of v1 to v2: (v1 dot v2) /{||v1||*||v2||}\"\n","  sumxx, sumxy, sumyy = 0, 0, 0\n","  for i in range(len(v1)):\n","    x = v1[i]; y = v2[i]\n","    sumxx += x*x\n","    sumyy += y*y\n","    sumxy += x*y\n","  return sumxy/math.sqrt(sumxx*sumyy)\n","\n","def get_similarity_score(X_vector, source):\n","    source_vector = X_vector[source]\n","    similarity_list = []\n","    for target_vector in X_vector:\n","        similarity_list.append(\n","            get_cosine_similarity(source_vector, target_vector))\n","    return similarity_list\n","\n","def get_top_n_similarity_news(similarity_score, n):\n","  import operator\n","  x = {i:v for i, v in enumerate(similarity_score)}\n","  sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n","  \n","  return list(reversed(sorted_x))[1:n+1]\n","\n","def get_accuracy(similarity_list, y_class, source_news):\n","  source_class = y_class[source_news]\n","  \n","  return sum([source_class == y_class[i[0]] for i in similarity_list]) / len(similarity_list)\n","\n","     \n","if __name__ == \"__main__\":\n","  dir_name = '/content/gdrive/My Drive/Python/news_data'\n","  file_list = get_file_list(dir_name)\n","  file_list = [os.path.join(dir_name, file_name) for file_name in file_list]\n","  \n","  X_text, y_class = get_contents(file_list)\n","  \n","  corpus = get_corpus_dict(X_text)\n","  print('Number of words : {0}'.format(len(corpus)))\n","  X_vector = get_count_vector(X_text, corpus)\n","  \n","  source_number = 10\n","  \n","  result = []\n","  \n","  for i in range(80):\n","    source_number = i\n","    \n","    similarity_score = get_similarity_score(X_vector, source_number)\n","    similarity_news = get_top_n_similarity_news(similarity_score, 10)\n","    accuracy_score = get_accuracy(similarity_news, y_class, source_number)\n","    result.append(accuracy_score)\n","\n","  print(sum(result)/ 80)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Number of words : 4024\n","0.6950000000000001\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sDC863qeT2IQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yF8E-biRMzcz","colab_type":"code","outputId":"6e6be1cf-5c30-4f3b-f987-24312d93fce2","executionInfo":{"status":"ok","timestamp":1562381849647,"user_tz":-540,"elapsed":2379,"user":{"displayName":"jinyoung song","photoUrl":"","userId":"03900198277628231796"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["!ls /content/gdrive"],"execution_count":0,"outputs":[{"output_type":"stream","text":["'My Drive'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tUGTY_9cNqUJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ec68cea8-1599-494b-b99c-3a38fa1fb2e4","executionInfo":{"status":"ok","timestamp":1562390949813,"user_tz":-540,"elapsed":1206,"user":{"displayName":"jinyoung song","photoUrl":"","userId":"03900198277628231796"}}},"source":["# News Categorization using sklearn\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer()\n","corpus = [\n","    'This is the first document.',\n","    'This is the second second document',\n","    'And the third one',\n","    'Is this the first document?',\n","]\n","\n","X = vectorizer.fit_transform(corpus)\n","X.toarray()\n","\n","vectorizer.get_feature_names()\n"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"sU91-Do_wepC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}